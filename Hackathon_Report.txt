Project Title: Offroad Semantic Scene Segmentation using DeepLabV3+
Hackathon: Startathon Hackathon 2026 – Offroad Segmentation Track

Team Name: Meticulous Masterminds

Member 1 : Deepak Yadav (25BCE10935) (Leader)
Member 2 : Ashutosh Pavaiya (25BCE10965)
Member 3 : Soumya Gupta (25BAI10628)
Member 4 : Aashka Saiwal (25BEY10001)
1. Problem Statement & Summary

The goal of this project is to train a semantic segmentation model that can understand off‑road desert scenes using Duality AI’s synthetic "digital twin" dataset. Each pixel in an RGB image must be classified into one of 10 classes:

	1. Trees
	2. Lush Bushes
	3. Dry Grass
	4. Dry Bushes
	5. Ground Clutter
	6. Flowers
	7. Logs
	8. Rocks
	9. Landscape (general ground)
	10. Sky

This capability is important for off‑road autonomy systems, which must distinguish traversable ground from obstacles, vegetation, and sky in complex outdoor environments.

We fine‑tuned a DeepLabV3+ model with a ResNet‑50 backbone on the provided training set, using a class‑aware loss and strong data augmentation. On the official validation split, our model achieved:

Validation mean IoU (mIoU): [VAL_MIOU]
Average inference time per 384×384 image: [AVG_MS] ms

These numbers were obtained using the test_offroad.py --split val script.

2. Dataset & Preprocessing

Dataset source:

Duality AI synthetic desert environment (digital twin)
Provided by the organizers as:
	Offroad_Segmentation_Training_Dataset\Offroad_Segmentation_Training_Dataset\
		train\Color_Images (2857 RGB images)
		train\Segmentation (uint16 masks)
		val\Color_Images (317 RGB images)
		val\Segmentation (uint16 masks)
	Offroad_Segmentation_testImages\Offroad_Segmentation_testImages\
		Color_Images
		Segmentation

Masks are stored as single‑channel 16‑bit PNGs. Each pixel contains one of the following raw IDs:

100 – Trees
200 – Lush Bushes
300 – Dry Grass
500 – Dry Bushes
550 – Ground Clutter
600 – Flowers
700 – Logs
800 – Rocks
7100 – Landscape
10000 – Sky

For training we map these raw IDs to contiguous class indices 0–9:

0 – Trees
1 – Lush Bushes
2 – Dry Grass
3 – Dry Bushes
4 – Ground Clutter
5 – Flowers
6 – Logs
7 – Rocks
8 – Landscape
9 – Sky

Any pixel with an ID outside this list is assigned label 255 and treated as ignore_index during loss and metric computation.

Preprocessing steps:

Load RGB images and convert from BGR (OpenCV) to RGB.
Load uint16 masks, then convert using the mapping above.
Resize both images and masks to 384×384 resolution to reduce GPU memory and speed up training.
Normalize images with ImageNet statistics (mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) to match the ResNet‑50 backbone pretraining.

3. Model & Training Methodology

Model architecture:

DeepLabV3+ with a ResNet‑50 backbone (torchvision implementation).
Backbone initialized with COCO pretrained weights.
Final classifier layer modified from 21 channels to 10 channels to match the required semantic classes.
Auxiliary classifier branch kept during training to improve gradient flow in intermediate layers.

Loss function and class imbalance:

We observed strong class imbalance: large areas labeled as Landscape and Sky, while Flowers and Logs occupy very few pixels.
To address this, we used a Focal Loss built on top of Cross‑Entropy with:
	ignore_index = 255 for unlabeled pixels.
	Focusing parameter gamma = 2.0.
Focal Loss reduces the weight of easy examples and focuses training on rare / difficult classes.

Data augmentation (Albumentations): 

RandomResizedCrop to 384×384 with scale range (0.7, 1.0) and slight aspect ratio jitter.
HorizontalFlip with probability 0.5.
RandomBrightnessContrast to simulate lighting changes.
HueSaturationValue to simulate color shifts in vegetation and ground.
GaussNoise and MotionBlur to mimic sensor noise and motion blur.

Optimization and training setup:

Optimizer: AdamW
	Learning rate: 1e‑4
	Weight decay: 1e‑4
LR scheduler: ReduceLROnPlateau (mode = "max") monitoring validation mIoU.
Batch size: 4
Input resolution: 384 × 384
Epochs: 2 effective epochs (limited by hackathon time).
Training device: NVIDIA GPU on local machine (PyTorch reports cuda = True).

The training script (train_offroad.py) logs after each epoch:

Training loss
Validation loss
Validation mean IoU
Per‑class IoU for each of the 10 classes
The best checkpoint (highest validation mIoU) is saved as best_model.pth.

4. Results & Performance

4.1 Validation Metrics

We evaluate the final model on the official validation split (317 images) using:

python test_offroad.py --split val

This script loads best_model.pth, runs inference on all validation images, and computes IoU for each class and the mean IoU across all classes.

Final validation results:

Mean IoU: [VAL_MIOU]

Per‑class IoU:

Trees: [IOU_TREES]
Lush Bushes: [IOU_LUSH]
Dry Grass: [IOU_DRY_GRASS]
Dry Bushes: [IOU_DRY_BUSHES]
Ground Clutter: [IOU_GROUND]
Flowers: [IOU_FLOWERS]
Logs: [IOU_LOGS]
Rocks: [IOU_ROCKS]
Landscape: [IOU_LANDSCAPE]
Sky: [IOU_SKY]

(Values copied directly from the console output of test_offroad.py.)

Typically, performance is highest on large and frequent classes such as Landscape and Sky, and lower on very small objects like Flowers and Logs, which occupy only a tiny fraction of pixels.

4.2 Inference Speed

The same script measures the average inference time per image:

Average inference time per 384×384 image: [AVG_MS] ms (on our GPU)

This is close to or below the recommended 50 ms target when further optimized (e.g., mixed precision, ONNX Runtime, or reduced resolution). It indicates that the model can be used in near real‑time perception pipelines.

4.3 Confusion Matrix

We also compute a 10×10 confusion matrix over all validation pixels (rows = ground truth class, columns = predicted class). The matrix highlights that most errors come from confusion between visually similar vegetation types (e.g., Dry Grass vs. Ground Clutter) and between small obstacles (Logs vs. Rocks). This confirms the strong class imbalance and motivates future work on targeted sampling and loss functions.

4.4 Qualitative Results

test_offroad.py also saves overlay visualizations into outputs\val\, where each file shows:

Original RGB image
Colorized segmentation mask overlaid at 50% transparency

Visual inspection shows that:

The model correctly segments major terrain boundaries (ground vs. sky).
Vegetation patches (bushes vs. grass) are often correctly separated.
Rocks and logs are detected in many scenes, although thin or heavily occluded objects remain challenging.

5. Challenges & How We Solved Them

a. Environment and Package Setup on Windows

Issue: Conda environment creation and activation failed due to Terms of Service prompts and PowerShell integration problems, which initially blocked installation of required libraries.
Solution: Instead of relying on the provided batch scripts, we used the base Anaconda environment and installed PyTorch, torchvision, albumentations, and other dependencies via pip. This simplified our setup and allowed us to focus on model development.

b. Non‑standard Label Encoding

Issue: Masks used 16‑bit integer IDs (e.g., 100, 200, 7100, 10000) rather than contiguous class indices. Directly training on these values caused incorrect learning and metrics.
Solution: We wrote a mapping function that converts raw IDs to indices 0–9 and sets all other pixels to ignore_index=255. This ensured that loss and IoU metrics are computed correctly for the 10 classes.

c. Class Imbalance (Flowers and Logs)

Issue: Small classes like Flowers and Logs appeared very rarely, so a standard Cross‑Entropy loss caused the model to focus mainly on large classes (Landscape, Sky).
Solution: We implemented Focal Loss to emphasize hard examples and minority classes. Combined with strong data augmentation (brightness, contrast, color jitter, noise), this improved IoU for rare categories.

d. Limited Training Time During Hackathon

Issue: Training DeepLabV3+ at full resolution for many epochs is computationally expensive, and we were limited by hackathon time (only a few hours).
Solution: We reduced the training resolution to 384×384 and trained for 2 epochs using a COCO pretrained backbone. This still yielded a reasonable mIoU while keeping the total training time manageable.

6. Future Work

If we had more time beyond the hackathon, we would extend this work in several ways:

a. Longer and Higher‑Resolution Training

Train for more epochs (e.g., 20–40) at higher input resolutions such as 512×512 or the original 960×540. This should improve boundary accuracy and small object segmentation.

b. Improved Losses and Sampling for Rare Classes

Experiment with Dice Loss, Tversky Loss, or class‑balanced Cross‑Entropy in combination with Focal Loss.
Use class‑balanced sampling or oversampling of images containing Flowers and Logs to further improve their IoU.

c. Domain Adaptation and Self‑Supervised Pretraining

Apply self‑supervised pretraining on unlabeled off‑road images and domain adaptation techniques to better generalize from synthetic data to real‑world off‑road scenes.

d. Deployment Optimization

Export the trained model to ONNX and run with ONNX Runtime or TensorRT.
Use mixed‑precision inference (FP16) and dynamic resizing for sub‑50 ms runtime at full resolution, suitable for real‑time robot or vehicle deployment.

7. How to Reproduce Our Results

Install Python and dependencies (see README.txt).
2. Ensure the dataset folders are placed exactly as provided by the organizers.
3. From the project root, run:
Training: python train_offroad.py
Evaluation: python test_offroad.py --split val
4. The script will save the best model as best_model.pth and generate overlay visualizations in the outputs folder.

This completes our submission for the Startathon 2026 Offroad Segmentation challenge.

End of Report
